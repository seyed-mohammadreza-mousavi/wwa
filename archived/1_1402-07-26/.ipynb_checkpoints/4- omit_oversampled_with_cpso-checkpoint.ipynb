{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aAmohammadrezaaA/Retinal-Vessel-Segmentation_A-Computer-Vision-Technique/blob/main/implementation_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d4E8HHv8sLCr"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8128\\1710360181.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtabulate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mConv2DTranspose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAdd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPool2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mUpSampling2D\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "import tabulate\n",
    "from tabulate import tabulate\n",
    "from prettytable import PrettyTable\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import concatenate, MaxPooling2D, DepthwiseConv2D, AveragePooling2D,Conv2DTranspose,Input,Add,Conv2D, BatchNormalization,LeakyReLU, Activation, MaxPool2D, Dropout, Flatten, Dense,UpSampling2D,Concatenate,Softmax, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "patch_size=48\n",
    "patch_num=1500\n",
    "patch_threshold=25\n",
    "BATCH_SIZE=64\n",
    "LR=0.0003\n",
    "channels=3\n",
    "\n",
    "input_shape = (patch_size, patch_size, channels)  # Adjusting the shape\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "# random input tensor for testing\n",
    "input_tensor = tf.random.normal((batch_size,) + input_shape)\n",
    "\n",
    "class LinearTransform(tf.keras.Model):\n",
    "  def __init__(self, name=\"LinearTransform\"):\n",
    "    super(LinearTransform, self).__init__(self,name=name)\n",
    "\n",
    "    self.conv_r=Conv2D(1,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.conv_g=Conv2D(1,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.conv_b=Conv2D(1,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.pool_rc=AveragePooling2D(pool_size=(patch_size,patch_size),strides=1)\n",
    "    self.pool_gc=AveragePooling2D(pool_size=(patch_size,patch_size),strides=1)\n",
    "    self.pool_bc=AveragePooling2D(pool_size=(patch_size,patch_size),strides=1)\n",
    "\n",
    "    self.bn=BatchNormalization()\n",
    "    self.sigmoid=Activation('sigmoid')\n",
    "    self.softmax=Activation('softmax')\n",
    "\n",
    "  def call(self, input,training=True):\n",
    "    r,g,b=input[:,:,:,0:1],input[:,:,:,1:2],input[:,:,:,2:3]\n",
    "\n",
    "    rs=self.conv_r(r)\n",
    "    gs=self.conv_g(g)\n",
    "    bs=self.conv_r(b)\n",
    "\n",
    "    rc=tf.reshape(self.pool_rc(rs),[-1,1])\n",
    "    gc=tf.reshape(self.pool_gc(gs),[-1,1])\n",
    "    bc=tf.reshape(self.pool_bc(bs),[-1,1])\n",
    "\n",
    "    merge=Concatenate(axis=-1)([rc,gc,bc])\n",
    "    merge=tf.expand_dims(merge,axis=1)\n",
    "    merge=tf.expand_dims(merge,axis=1)\n",
    "    merge=self.softmax(merge)\n",
    "    merge=tf.repeat(merge,repeats=48,axis=2)\n",
    "    merge=tf.repeat(merge,repeats=48,axis=1)\n",
    "\n",
    "    r=r*(1+self.sigmoid(rs))\n",
    "    g=g*(1+self.sigmoid(gs))\n",
    "    b=b*(1+self.sigmoid(bs))\n",
    "\n",
    "    output=self.bn(merge[:,:,:,0:1]*r+merge[:,:,:,1:2]*g+merge[:,:,:,2:3]*b,training=training)\n",
    "    return output\n",
    "\n",
    "class ResBlock(tf.keras.Model):\n",
    "  def __init__(self,out_ch,residual_path=False,stride=1):\n",
    "    super(ResBlock,self).__init__(self)\n",
    "    self.residual_path=residual_path\n",
    "\n",
    "    self.conv1=Conv2D(out_ch,kernel_size=3,strides=stride,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.bn1=BatchNormalization()\n",
    "    self.relu1=LeakyReLU()#Activation('leaky_relu')\n",
    "\n",
    "    self.conv2=Conv2D(out_ch,kernel_size=3,strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.bn2=BatchNormalization()\n",
    "\n",
    "    if residual_path:\n",
    "      self.conv_shortcut=Conv2D(out_ch,kernel_size=1,strides=stride,padding='same',use_bias=False)\n",
    "      self.bn_shortcut=BatchNormalization()\n",
    "\n",
    "    self.relu2=LeakyReLU()#Activation('leaky_relu')\n",
    "\n",
    "  def call(self,x,training=True):\n",
    "    xs=self.relu1(self.bn1(self.conv1(x),training=training))\n",
    "    xs=self.bn2(self.conv2(xs),training=training)\n",
    "\n",
    "    if self.residual_path:\n",
    "      x=self.bn_shortcut(self.conv_shortcut(x),training=training)\n",
    "    #print(x.shape,xs.shape)\n",
    "    xs=x+xs\n",
    "    return self.relu2(xs)\n",
    "\n",
    "class Unet(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Unet,self).__init__(self)\n",
    "    self.conv_init=LinearTransform()\n",
    "    self.resinit=ResBlock(16,residual_path=True)\n",
    "    self.up_sample=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resup=ResBlock(32,residual_path=True)\n",
    "\n",
    "    self.pool1=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    self.resblock_down1=ResBlock(64,residual_path=True)\n",
    "    self.resblock_down11=ResBlock(64,residual_path=False)\n",
    "    self.pool2=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    self.resblock_down2=ResBlock(128,residual_path=True)\n",
    "    self.resblock_down21=ResBlock(128,residual_path=False)\n",
    "    self.pool3=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    self.resblock_down3=ResBlock(256,residual_path=True)\n",
    "    self.resblock_down31=ResBlock(256,residual_path=False)\n",
    "    self.pool4=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    self.resblock=ResBlock(512,residual_path=True)\n",
    "\n",
    "    self.unpool3=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resblock_up3=ResBlock(256,residual_path=True)\n",
    "    self.resblock_up31=ResBlock(256,residual_path=False)\n",
    "\n",
    "    self.unpool2=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resblock_up2=ResBlock(128,residual_path=True)\n",
    "    self.resblock_up21=ResBlock(128,residual_path=False)\n",
    "\n",
    "    self.unpool1=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resblock_up1=ResBlock(64,residual_path=True)\n",
    "\n",
    "    self.unpool_final=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.resblock2=ResBlock(32,residual_path=True)\n",
    "\n",
    "    self.pool_final=MaxPool2D(pool_size=(2,2))\n",
    "    self.resfinal=ResBlock(32)\n",
    "\n",
    "    self.conv_final=Conv2D(1,kernel_size=1,strides=1,padding='same',use_bias=False)\n",
    "    self.bn_final=BatchNormalization()\n",
    "    self.act=Activation('sigmoid')\n",
    "\n",
    "  def call(self,x,training=True):\n",
    "    x_linear=self.conv_init(x,training=training)\n",
    "    x=self.resinit(x_linear,training=training)\n",
    "    x=self.up_sample(x)\n",
    "    x=self.resup(x,training=training)\n",
    "\n",
    "    stage1=self.pool1(x)\n",
    "    stage1=self.resblock_down1(stage1,training=training)\n",
    "    stage1=self.resblock_down11(stage1,training=training)\n",
    "\n",
    "    stage2=self.pool2(stage1)\n",
    "    stage2=self.resblock_down2(stage2,training=training)\n",
    "    stage2=self.resblock_down21(stage2,training=training)\n",
    "\n",
    "    stage3=self.pool3(stage2)\n",
    "    stage3=self.resblock_down3(stage3,training=training)\n",
    "    stage3=self.resblock_down31(stage3,training=training)\n",
    "\n",
    "    stage4=self.pool4(stage3)\n",
    "    stage4=self.resblock(stage4,training=training)\n",
    "\n",
    "    stage3=Concatenate(axis=3)([stage3,self.unpool3(stage4)])\n",
    "    stage3=self.resblock_up3(stage3,training=training)\n",
    "    stage3=self.resblock_up31(stage3,training=training)\n",
    "\n",
    "    stage2=Concatenate(axis=3)([stage2,self.unpool2(stage3)])\n",
    "    stage2=self.resblock_up2(stage2,training=training)\n",
    "    stage2=self.resblock_up21(stage2,training=training)\n",
    "\n",
    "    stage1=Concatenate(axis=3)([stage1,self.unpool1(stage2)])\n",
    "    stage1=self.resblock_up1(stage1,training=training)\n",
    "\n",
    "    x=Concatenate(axis=3)([x,self.unpool_final(stage1)])\n",
    "    x=self.resblock2(x,training=training)\n",
    "\n",
    "    x=self.pool_final(x)\n",
    "    x=self.resfinal(x,training=training)\n",
    "\n",
    "    seg_result=self.act(self.bn_final(self.conv_final(x),training=training))\n",
    "\n",
    "    return x_linear,seg_result\n",
    "class Unet2(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Unet2,self).__init__(self)\n",
    "    self.conv_init=LinearTransform()\n",
    "    #self.resinit=ResBlock(16,residual_path=True)\n",
    "    self.resinit=Conv2D(16,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.up_sample=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resup=ResBlock(32,residual_path=True)\n",
    "    self.resup=Conv2D(32,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.pool1=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    #self.resblock_down1=ResBlock(64,residual_path=True)\n",
    "    self.resblock_down1=Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_down11=ResBlock(64,residual_path=False)\n",
    "    self.resblock_down11=Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.pool2=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    #self.resblock_down2=ResBlock(128,residual_path=True)\n",
    "    self.resblock_down2=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_down21=ResBlock(128,residual_path=False)\n",
    "    self.resblock_down21=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.pool3=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    #self.resblock_down3=ResBlock(256,residual_path=True)\n",
    "    self.resblock_down3=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_down31=ResBlock(256,residual_path=False)\n",
    "    self.resblock_down31=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    self.pool4=MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "    #self.resblock=ResBlock(512,residual_path=True)\n",
    "    self.resblock=Conv2D(512,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.unpool3=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resblock_up3=ResBlock(256,residual_path=True)\n",
    "    self.resblock_up3=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_up31=ResBlock(256,residual_path=False)\n",
    "    self.resblock_up31=Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.unpool2=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resblock_up2=ResBlock(128,residual_path=True)\n",
    "    self.resblock_up2=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "    #self.resblock_up21=ResBlock(128,residual_path=False)\n",
    "    self.resblock_up21=Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.unpool1=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resblock_up1=ResBlock(64,residual_path=True)\n",
    "    self.resblock_up1=Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.unpool_final=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    #self.resblock2=ResBlock(32,residual_path=True)\n",
    "    self.resblock2=Conv2D(32,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.pool_final=MaxPool2D(pool_size=(2,2))\n",
    "    #self.resfinal=ResBlock(32)\n",
    "    self.resfinal=Conv2D(32,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    self.conv_final=Conv2D(1,kernel_size=1,strides=1,padding='same',use_bias=False)\n",
    "    self.bn_final=BatchNormalization()\n",
    "    self.act=Activation('sigmoid')\n",
    "\n",
    "  def call(self,x,training=True):\n",
    "    x_linear=self.conv_init(x,training=training)\n",
    "    x=self.resinit(x_linear,training=training)\n",
    "    x=self.up_sample(x)\n",
    "    x=self.resup(x,training=training)\n",
    "\n",
    "    stage1=self.pool1(x)\n",
    "    stage1=self.resblock_down1(stage1,training=training)\n",
    "    stage1=self.resblock_down11(stage1,training=training)\n",
    "\n",
    "    stage2=self.pool2(stage1)\n",
    "    stage2=self.resblock_down2(stage2,training=training)\n",
    "    stage2=self.resblock_down21(stage2,training=training)\n",
    "\n",
    "    stage3=self.pool3(stage2)\n",
    "    stage3=self.resblock_down3(stage3,training=training)\n",
    "    stage3=self.resblock_down31(stage3,training=training)\n",
    "\n",
    "    stage4=self.pool4(stage3)\n",
    "    stage4=self.resblock(stage4,training=training)\n",
    "\n",
    "    stage3=Concatenate(axis=3)([stage3,self.unpool3(stage4)])\n",
    "    stage3=self.resblock_up3(stage3,training=training)\n",
    "    stage3=self.resblock_up31(stage3,training=training)\n",
    "\n",
    "    stage2=Concatenate(axis=3)([stage2,self.unpool2(stage3)])\n",
    "    stage2=self.resblock_up2(stage2,training=training)\n",
    "    stage2=self.resblock_up21(stage2,training=training)\n",
    "\n",
    "    stage1=Concatenate(axis=3)([stage1,self.unpool1(stage2)])\n",
    "    stage1=self.resblock_up1(stage1,training=training)\n",
    "\n",
    "    x=Concatenate(axis=3)([x,self.unpool_final(stage1)])\n",
    "    x=self.resblock2(x,training=training)\n",
    "\n",
    "    x=self.pool_final(x)\n",
    "    x=self.resfinal(x,training=training)\n",
    "\n",
    "    seg_result=self.act(self.bn_final(self.conv_final(x),training=training))\n",
    "\n",
    "    return x_linear,seg_result\n",
    "\n",
    "class Unet3(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Unet3, self).__init__()\n",
    "\n",
    "        # Define the encoder layers\n",
    "        #self.encoder_conv_init=LinearTransform()\n",
    "        self.encoder_conv_init=Conv2D(1, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv1 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool1 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.encoder_conv2 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool2 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.encoder_conv3 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool3 = MaxPool2D(pool_size=(2, 2))\n",
    "\n",
    "        # Define the bottleneck layer\n",
    "        self.bottleneck_conv = Conv2D(512, 3, activation='relu', padding='same')\n",
    "\n",
    "        # Define the decoder layers\n",
    "        self.decoder_upsample1 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv4 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.decoder_upsample2 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv5 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.decoder_upsample3 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv6 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.decoder_output = Conv2D(1, 3, activation='sigmoid', padding='same')\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Encoder\n",
    "        x_linear=self.encoder_conv_init(x, training=training)\n",
    "        enc1 = self.encoder_conv1(x_linear, training=training)\n",
    "        enc1_pool = self.encoder_pool1(enc1)\n",
    "        enc2 = self.encoder_conv2(enc1_pool, training=training)\n",
    "        enc2_pool = self.encoder_pool2(enc2)\n",
    "        enc3 = self.encoder_conv3(enc2_pool, training=training)\n",
    "        enc3_pool = self.encoder_pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck_conv(enc3_pool, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        dec1 = self.decoder_upsample1(bottleneck)\n",
    "        dec1 = Concatenate()([dec1, enc3])\n",
    "        dec1 = self.decoder_conv4(dec1, training=training)\n",
    "        dec2 = self.decoder_upsample2(dec1)\n",
    "        dec2 = Concatenate()([dec2, enc2])\n",
    "        dec2 = self.decoder_conv5(dec2, training=training)\n",
    "        dec3 = self.decoder_upsample3(dec2)\n",
    "        dec3 = Concatenate()([dec3, enc1])\n",
    "        dec3 = self.decoder_conv6(dec3, training=training)\n",
    "\n",
    "        # Output\n",
    "        seg_result = self.decoder_output(dec3, training=training)\n",
    "        return x_linear,seg_result\n",
    "\n",
    "class Unet4(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Unet4, self).__init__()\n",
    "\n",
    "        # Define the encoder layers\n",
    "        self.encoder_conv_init=LinearTransform()\n",
    "        #self.encoder_conv_init=Conv2D(1, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv1 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool1 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.encoder_conv2 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool2 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.encoder_conv3 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.encoder_pool3 = MaxPool2D(pool_size=(2, 2))\n",
    "\n",
    "        # Define the bottleneck layer\n",
    "        self.bottleneck_conv = Conv2D(512, 3, activation='relu', padding='same')\n",
    "\n",
    "        # Define the decoder layers\n",
    "        self.decoder_upsample1 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv4 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.decoder_upsample2 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv5 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.decoder_upsample3 = UpSampling2D(size=(2, 2))\n",
    "        self.decoder_conv6 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.decoder_output = Conv2D(1, 3, activation='sigmoid', padding='same')\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Encoder\n",
    "        x_linear=self.encoder_conv_init(x, training=training)\n",
    "        enc1 = self.encoder_conv1(x_linear, training=training)\n",
    "        enc1_pool = self.encoder_pool1(enc1)\n",
    "        enc2 = self.encoder_conv2(enc1_pool, training=training)\n",
    "        enc2_pool = self.encoder_pool2(enc2)\n",
    "        enc3 = self.encoder_conv3(enc2_pool, training=training)\n",
    "        enc3_pool = self.encoder_pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck_conv(enc3_pool, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        dec1 = self.decoder_upsample1(bottleneck)\n",
    "        dec1 = Concatenate()([dec1, enc3])\n",
    "        dec1 = self.decoder_conv4(dec1, training=training)\n",
    "        dec2 = self.decoder_upsample2(dec1)\n",
    "        dec2 = Concatenate()([dec2, enc2])\n",
    "        dec2 = self.decoder_conv5(dec2, training=training)\n",
    "        dec3 = self.decoder_upsample3(dec2)\n",
    "        dec3 = Concatenate()([dec3, enc1])\n",
    "        dec3 = self.decoder_conv6(dec3, training=training)\n",
    "\n",
    "        # Output\n",
    "        seg_result = self.decoder_output(dec3, training=training)\n",
    "        return x_linear,seg_result\n",
    "\n",
    "class Unet5(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Unet5, self).__init__()\n",
    "\n",
    "        # Define the encoder layers\n",
    "        #self.encoder_conv_init=LinearTransform()\n",
    "        self.encoder_conv_init=Conv2D(1, 3, activation='relu', padding='same')\n",
    "        #self.encoder_conv1 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv1 = ResBlock(32,residual_path=True)\n",
    "        self.encoder_pool1 = MaxPool2D(pool_size=(2, 2))\n",
    "        #self.encoder_conv2 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv2 = ResBlock(64,residual_path=True)\n",
    "        self.encoder_pool2 = MaxPool2D(pool_size=(2, 2))\n",
    "        #self.encoder_conv3 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv3 = ResBlock(128,residual_path=True)\n",
    "        self.encoder_pool3 = MaxPool2D(pool_size=(2, 2))\n",
    "\n",
    "        # Define the bottleneck layer\n",
    "        #self.bottleneck_conv = Conv2D(512, 3, activation='relu', padding='same')\n",
    "        self.bottleneck_conv = ResBlock(512,residual_path=True)\n",
    "\n",
    "        # Define the decoder layers\n",
    "        self.decoder_upsample1 = UpSampling2D(size=(2, 2))\n",
    "        #self.decoder_conv4 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.decoder_conv4 = ResBlock(128,residual_path=True)\n",
    "        self.decoder_upsample2 = UpSampling2D(size=(2, 2))\n",
    "        #self.decoder_conv5 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.decoder_conv5 = ResBlock(64,residual_path=True)\n",
    "        self.decoder_upsample3 = UpSampling2D(size=(2, 2))\n",
    "        #self.decoder_conv6 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.decoder_conv6 = ResBlock(32,residual_path=True)\n",
    "        #self.decoder_output = Conv2D(1, 3, activation='sigmoid', padding='same')\n",
    "        self.decoder_output = ResBlock(1,residual_path=True)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Encoder\n",
    "        x_linear=self.encoder_conv_init(x, training=training)\n",
    "        enc1 = self.encoder_conv1(x_linear, training=training)\n",
    "        enc1_pool = self.encoder_pool1(enc1)\n",
    "        enc2 = self.encoder_conv2(enc1_pool, training=training)\n",
    "        enc2_pool = self.encoder_pool2(enc2)\n",
    "        enc3 = self.encoder_conv3(enc2_pool, training=training)\n",
    "        enc3_pool = self.encoder_pool3(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck_conv(enc3_pool, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        dec1 = self.decoder_upsample1(bottleneck)\n",
    "        dec1 = Concatenate()([dec1, enc3])\n",
    "        dec1 = self.decoder_conv4(dec1, training=training)\n",
    "        dec2 = self.decoder_upsample2(dec1)\n",
    "        dec2 = Concatenate()([dec2, enc2])\n",
    "        dec2 = self.decoder_conv5(dec2, training=training)\n",
    "        dec3 = self.decoder_upsample3(dec2)\n",
    "        dec3 = Concatenate()([dec3, enc1])\n",
    "        dec3 = self.decoder_conv6(dec3, training=training)\n",
    "\n",
    "        # Output\n",
    "        seg_result = self.decoder_output(dec3, training=training)\n",
    "        return x_linear,seg_result\n",
    "\n",
    "# Define the ShuffleNet block as a custom layer\n",
    "class ShuffleNetBlock(Layer):\n",
    "    def __init__(self, out_channels, groups=4, **kwargs):\n",
    "        super(ShuffleNetBlock, self).__init__(**kwargs)\n",
    "        self.out_channels = out_channels\n",
    "        self.groups = groups\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self.out_channels % self.groups != 0:\n",
    "            raise ValueError(\"The number of filters must be evenly divisible by the number of groups.\")\n",
    "    \n",
    "        self.filters_per_group = self.out_channels // self.groups\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # 1x1 Convolution\n",
    "        x = DepthwiseConv2D((3, 3), padding='same', activation='relu')(inputs)\n",
    "        x = Conv2D(self.filters_per_group, (1, 1), activation='relu', padding='same')(x)\n",
    "    \n",
    "        # Grouped Convolution\n",
    "        splits = tf.split(x, num_or_size_splits=self.groups, axis=-1)\n",
    "        group_outputs = []\n",
    "        for i in range(self.groups):\n",
    "            group = Conv2D(self.filters_per_group, (3, 3), activation='relu', padding='same')(splits[i])\n",
    "            group_outputs.append(group)\n",
    "    \n",
    "        # Concatenate the group outputs\n",
    "        x = concatenate(group_outputs, axis=-1)\n",
    "    \n",
    "        # 1x1 Convolution to adjust the number of channels\n",
    "        x = Conv2D(self.out_channels, (1, 1), activation='relu', padding='same')(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the U-Net model with ShuffleNet blocks as a custom model\n",
    "class Unet6(Model):\n",
    "    def __init__(self, input_shape=(128, 128, 3), num_classes=1):\n",
    "        super(Unet6, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv0 = Conv2D(1, 3, activation='relu', padding='same')\n",
    "        self.conv1 = ShuffleNetBlock(64)\n",
    "        self.pool1 = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv2 = ShuffleNetBlock(128)\n",
    "        self.pool2 = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv3 = ShuffleNetBlock(256)\n",
    "        self.pool3 = MaxPooling2D(pool_size=(2, 2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ShuffleNetBlock(512)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up4 = UpSampling2D(size=(2, 2))\n",
    "        self.concat4 = concatenate\n",
    "        self.conv4 = ShuffleNetBlock(256)\n",
    "        self.up5 = UpSampling2D(size=(2, 2))\n",
    "        self.concat5 = concatenate\n",
    "        self.conv5 = ShuffleNetBlock(128)\n",
    "        self.up6 = UpSampling2D(size=(2, 2))\n",
    "        self.concat6 = concatenate\n",
    "        self.conv6 = ShuffleNetBlock(64)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = Conv2D(num_classes, (1, 1), activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Encoder\n",
    "        conv0 = self.conv0(inputs)\n",
    "        conv1 = self.conv1(conv0)\n",
    "        pool1 = self.pool1(conv1)\n",
    "        conv2 = self.conv2(pool1)\n",
    "        pool2 = self.pool2(conv2)\n",
    "        conv3 = self.conv3(pool2)\n",
    "        pool3 = self.pool3(conv3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(pool3)\n",
    "        \n",
    "        # Decoder\n",
    "        up4 = self.up4(bottleneck)\n",
    "        concat4 = self.concat4([conv3, up4], axis=-1)\n",
    "        conv4 = self.conv4(concat4)\n",
    "        up5 = self.up5(conv4)\n",
    "        concat5 = self.concat5([conv2, up5], axis=-1)\n",
    "        conv5 = self.conv5(concat5)\n",
    "        up6 = self.up6(conv5)\n",
    "        concat6 = self.concat6([conv1, up6], axis=-1)\n",
    "        conv6 = self.conv6(concat6)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(conv6)\n",
    "        \n",
    "        return conv0, output\n",
    "\n",
    "\n",
    "class custom1(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(custom1, self).__init__()\n",
    "\n",
    "        # Define the encoder layers\n",
    "        #self.encoder_conv_init=Conv2D(1, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv_init=LinearTransform()\n",
    "        #self.encoder_conv1 = ResBlock(32,residual_path=True)\n",
    "        self.encoder_conv1 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        #self.encoder_conv2 = ResBlock(64,residual_path=True)\n",
    "        self.encoder_conv2 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv3 = Conv2DTranspose(filters=32, kernel_size=(3, 3), strides=(2, 2), padding='same')\n",
    "        self.encoder_conv4 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv5 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv6 = Conv2D(64, 3, (2, 2), activation='relu', padding='same')\n",
    "        self.encoder_conv7 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        #self.encoder_conv8 = ResBlock(64,residual_path=True)\n",
    "        self.encoder_conv8 = Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv9 = Conv2D(128, 3, (2, 2), activation='relu', padding='same')\n",
    "        self.encoder_conv10 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv11 = Conv2D(128, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv12 = Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), padding='same')\n",
    "        self.encoder_conv13 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv14 = Conv2D(32, 3, activation='relu', padding='same')\n",
    "        self.encoder_conv15 = Conv2D(1, 1, activation='relu', padding='same')   \n",
    "        self.final = Activation('tanh')\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Encoder\n",
    "        x_linear=self.encoder_conv_init(x, training=training)\n",
    "        ed = self.encoder_conv1(x_linear, training=training)\n",
    "        eds1 = self.encoder_conv2(ed, training=training)\n",
    "        ed = self.encoder_conv3(eds1, training=training)\n",
    "        ed = self.encoder_conv4(ed, training=training)\n",
    "        ed = self.encoder_conv5(ed, training=training)\n",
    "        ed = self.encoder_conv6(ed, training=training)\n",
    "        ed = Concatenate(axis=3)([ed,eds1])\n",
    "        ed = self.encoder_conv7(ed, training=training)\n",
    "        eds2 = self.encoder_conv8(ed, training=training)\n",
    "        ed = self.encoder_conv9(eds2, training=training)\n",
    "        ed = self.encoder_conv10(ed, training=training)\n",
    "        ed = self.encoder_conv11(ed, training=training)\n",
    "        ed = self.encoder_conv12(ed, training=training)\n",
    "        ed = Concatenate(axis=3)([ed,eds2])\n",
    "        ed = self.encoder_conv13(ed, training=training)\n",
    "        ed = self.encoder_conv14(ed, training=training)\n",
    "        ed = self.encoder_conv15(ed, training=training)\n",
    "        seg_result = self.final(ed, training=training)\n",
    "\n",
    "\n",
    "        return x_linear,seg_result\n",
    "\n",
    "class custom2(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(custom2,self).__init__(self)\n",
    "    self.conv_init=LinearTransform()#128*128*1\n",
    "    self.convd111=Conv2D(3,kernel_size=(3, 3),strides=(1, 1),padding='same',activation='relu', data_format=\"channels_last\")#128*128*3\n",
    "    self.convd1=Conv2D(64,kernel_size=(3, 3),strides=(1, 1),padding='same',activation='relu', data_format=\"channels_last\")#128*128*64\n",
    "    self.lcm11d1=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12d1=Conv2D(64,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.mpd1=MaxPool2D(pool_size=(2,2))#64*64*64\n",
    "\n",
    "    self.lcm11d2=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12d2=Conv2D(128,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")#64*64*128\n",
    "    self.lcm21d2=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22d2=Conv2D(128,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.mpd2=MaxPool2D(pool_size=(2,2))#32*32*128\n",
    "\n",
    "    self.lcm11d3=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12d3=Conv2D(256,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")#64*64*256\n",
    "    self.lcm21d3=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22d3=Conv2D(256,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")#64*64*256\n",
    "    self.mpd3=MaxPool2D(pool_size=(2,2))#32*32*256\n",
    "\n",
    "    self.lcm11d4=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12d4=Conv2D(512,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")#32*32*512\n",
    "    self.lcm21d4=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22d4=Conv2D(512,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")#32*32*512\n",
    "    self.mpd4=MaxPool2D(pool_size=(2,2))#16*16*512\n",
    "\n",
    "    self.lcm11d5=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12d5=Conv2D(512,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.lcm21d5=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22d5=Conv2D(512,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.mpd5=MaxPool2D(pool_size=(2,2))#16*16*512\n",
    "\n",
    "    self.lcm11d6=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12d6=Conv2D(1024,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")#16*16*1024\n",
    "    self.lcm21d6=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22d6=Conv2D(1024,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.mpd6=MaxPool2D(pool_size=(2,2))#8*8*1024\n",
    "\n",
    "    self.upe1=UpSampling2D(size=(2,2),interpolation='bilinear')#16*16*1024\n",
    "    self.lcm11e1=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12e1=Conv2D(512,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")#16*16*512\n",
    "    self.lcm21e1=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22e1=Conv2D(512,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")#16*16*512\n",
    "\n",
    "    self.upe2=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.lcm11e2=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12e2=Conv2D(512,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.lcm21e2=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22e2=Conv2D(512,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "\n",
    "    self.upe3=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.lcm11e3=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12e3=Conv2D(256,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.lcm21e3=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22e3=Conv2D(256,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "\n",
    "    self.upe4=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.lcm11e4=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12e4=Conv2D(128,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.lcm21e4=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22e4=Conv2D(128,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "\n",
    "    self.upe5=UpSampling2D(size=(2,2),interpolation='bilinear')\n",
    "    self.lcm11e5=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm12e5=Conv2D(64,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "    self.lcm21e5=DepthwiseConv2D(kernel_size=(3, 3), strides=(1, 1), padding='same')\n",
    "    self.lcm22e5=Conv2D(64,kernel_size=(1, 1),strides=1,padding='same', use_bias=False,data_format=\"channels_last\")\n",
    "\n",
    "    self.out=Conv2D(1,kernel_size=(1, 1),strides=1,padding='same',activation='softmax', use_bias=False,data_format=\"channels_last\")\n",
    "\n",
    "\n",
    "  def call(self,x,training=True):\n",
    "    #x_linear = self.conv_init(x, training=training)\n",
    "    #x_l2=self.convd111(x_linear, training=training)\n",
    "    x_linear=self.convd1(x, training=training)#128*128*64\n",
    "    x1d1=self.lcm11d1(x_linear, training=training)\n",
    "    x2d1=self.lcm12d1(x1d1, training=training)#128*128*64\n",
    "    x3d1=self.mpd1(x2d1)\n",
    "\n",
    "    x1d2=self.lcm11d2(x3d1, training=training)\n",
    "    x2d2=self.lcm12d2(x1d2, training=training)#64*64*128\n",
    "    x3d2=self.lcm21d2(x2d2, training=training)\n",
    "    x4d2=self.lcm22d2(x3d2, training=training)\n",
    "    x5d2=self.mpd2(x4d2)\n",
    "\n",
    "    x1d3=self.lcm11d3(x5d2, training=training)\n",
    "    x2d3=self.lcm12d3(x1d3, training=training)#32*32*256\n",
    "    x3d3=self.lcm21d3(x2d3, training=training)\n",
    "    x4d3=self.lcm22d3(x3d3, training=training)\n",
    "    x5d3=self.mpd3(x4d3)\n",
    "\n",
    "    x1d4=self.lcm11d4(x5d3, training=training)\n",
    "    x2d4=self.lcm12d4(x1d4, training=training)#16*16*512\n",
    "    x3d4=self.lcm21d4(x2d4, training=training)\n",
    "    x4d4=self.lcm22d4(x3d4, training=training)\n",
    "    x5d4=self.mpd4(x4d4)\n",
    "\n",
    "    x1d5=self.lcm11d5(x5d4, training=training)\n",
    "    x2d5=self.lcm12d5(x1d5, training=training)#8*8*512\n",
    "    x3d5=self.lcm21d5(x2d5, training=training)\n",
    "    x4d5=self.lcm22d5(x3d5, training=training)\n",
    "    x5d5=self.mpd5(x4d5)\n",
    "\n",
    "    x1d6=self.lcm11d6(x5d5, training=training)\n",
    "    x2d6=self.lcm12d6(x1d6, training=training)#4*4*1024\n",
    "    x3d6=self.lcm21d6(x2d6, training=training)\n",
    "    x4d6=self.lcm22d6(x3d6, training=training)\n",
    "\n",
    "    x1e1=self.upe1(x4d6)\n",
    "    x2e1=self.lcm11e1(x1e1, training=training)\n",
    "    x3e1=self.lcm12e1(x2e1, training=training)#8*8*512\n",
    "    x4e1=self.lcm21e1(x3e1, training=training)\n",
    "    x5e1=self.lcm22e1(x4e1, training=training)\n",
    "    x6e1=Concatenate(axis=-1)([x5e1,x4d5])\n",
    "\n",
    "    x1e2=self.upe2(x6e1, training=training)\n",
    "    x2e2=self.lcm11e2(x1e2, training=training)\n",
    "    x3e2=self.lcm12e2(x2e2, training=training)#16*16\n",
    "    x4e2=self.lcm21e2(x3e2, training=training)\n",
    "    x5e2=self.lcm22e2(x4e2, training=training)\n",
    "    x6e2=Concatenate(axis=-1)([x5e2,x4d4])\n",
    "\n",
    "    x1e3=self.upe3(x6e2, training=training)\n",
    "    x2e3=self.lcm11e3(x1e3, training=training)\n",
    "    x3e3=self.lcm12e3(x2e3, training=training)#32*32\n",
    "    x4e3=self.lcm21e3(x3e3, training=training)\n",
    "    x5e3=self.lcm22e3(x4e3, training=training)\n",
    "    x6e3=Concatenate(axis=-1)([x5e3,x4d3])\n",
    "\n",
    "    x1e4=self.upe4(x6e3, training=training)\n",
    "    x2e4=self.lcm11e4(x1e4, training=training)\n",
    "    x3e4=self.lcm12e4(x2e4, training=training)#64*64\n",
    "    x4e4=self.lcm21e4(x3e4, training=training)\n",
    "    x5e4=self.lcm22e4(x4e4, training=training)\n",
    "    x6e5=Concatenate(axis=-1)([x5e4,x4d2])\n",
    "\n",
    "    x1e5=self.upe5(x6e5, training=training)\n",
    "    x2e5=self.lcm11e5(x1e5, training=training)\n",
    "    x3e5=self.lcm12e5(x2e5, training=training)#128*128\n",
    "    x4e5=self.lcm21e5(x3e5, training=training)\n",
    "    x5e5=self.lcm22e5(x4e5, training=training)\n",
    "    x6e5=Concatenate(axis=-1)([x5e5,x2d1])\n",
    "\n",
    "    seg_result=self.out(x6e5, training=training)\n",
    "\n",
    "    return x_linear,seg_result\n",
    "\n",
    "cols = [\"#\", \"patch_size\", \"patch_num\", \"patch_threshold\", \"batch_size\", \"learning_rate\"]\n",
    "rows = [[ \"value\", patch_size, patch_num, patch_threshold, BATCH_SIZE, LR],\n",
    "        [\"comment\", \"(48*48) windows\", \"number of windows\", \"threshold for the patch, the smaller threshoold, the less vessel in the patch\", \"batch\", \"LR\"]]\n",
    "\n",
    "print(tabulate(rows, headers=cols,tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOUS51M47Wtv",
    "outputId": "f1065238-617a-4b73-c3bb-3be7b80d7f4e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Unet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8128\\2523024262.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_unet1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_unet2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUnet2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel_unet3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUnet3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_unet4\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUnet4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlinear_output1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_result1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_unet1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Unet' is not defined"
     ]
    }
   ],
   "source": [
    "#model_unet1=Unet()\n",
    "#model_unet2=Unet2()\n",
    "#model_unet3=Unet3()\n",
    "#model_unet4=Unet4()\n",
    "#model_unet5=Unet5()\n",
    "#model_custom1=custom1()\n",
    "#model_custom2=custom2()\n",
    "#linear_output1, seg_result1 = model_unet1(input_tensor)\n",
    "#linear_output2, seg_result2 = model_unet2(input_tensor)\n",
    "#linear_output3, seg_result3 = model_unet3(input_tensor)\n",
    "#linear_output4, seg_result4 = model_unet4(input_tensor)\n",
    "#linear_output5, seg_result5 = model_unet5(input_tensor)\n",
    "#linear_outputc1, seg_resultc1 = model_custom1(input_tensor)\n",
    "#linear_outputc2, seg_resultc2 = model_custom2(input_tensor)\n",
    "#model.summary(line_length=110)\n",
    "#model_unet1.count_params()\n",
    "#model_unet2.count_params()\n",
    "#model_unet3.count_params()\n",
    "\n",
    "#Unet1 binary crossentropy loss\n",
    "train_loss_bc=0.005448818;train_acc_bc=0.9918593;train_f1_bc=0.6594059;train_sp_bc=0.9915452;train_se_bc=0.9915452;train_precision_bc=0.9914305;train_auroc_bc=0.9915449\n",
    "val_loss_bc=0.28848;val_acc_bc=0.85171;val_f1_bc=0.53667;val_sp_bc=0.90131;val_se_bc=0.90131;val_precision_bc=0.90225;val_auroc_bc=0.90130\n",
    "trained_till_epoch_bc=120; lowest_val_loss_on_epoch_bc=44; highest_val_acc_on_epoch_bc=96; highest_val_f1_on_epoch_bc=96; highest_val_sp_on_epoch_bc=96; highest_val_se_on_epoch_bc=96;highest_val_prec_on_epoch_bc=96;highest_val_auroc_on_epoch_bc=96\n",
    "#Unet1 dice loss \n",
    "train_loss_d=0.0782; train_acc_d=0.9218; train_f1_d=0.5979; train_sp_d=0.8926; train_se_d=0.8926; train_precision_d=0.8920; train_auroc_d=0.8926\n",
    "val_loss_d=0.1992; val_acc_d=0.8008; val_f1_d=0.4933; val_sp_d=0.8016; val_se_d=0.8016; val_precision_d=0.8011; val_auroc_d=0.8016\n",
    "trained_till_epoch_d=\"Nan\"; lowest_val_loss_on_epoch_d=\"Nan\"\n",
    "# Unet1 focal loss\n",
    "train_loss_f=170.6789; train_acc_f=0.8527; train_f1_f=0.5390; train_sp_f=0.7866; train_se_f=0.7040; train_precision_f=0.7859; train_auroc_f=0.7866\n",
    "val_loss_f=4232.2256; val_acc_f=0.7367; val_f1_f=0.4396; val_sp_f=0.7040; val_se_f=0.8016; val_precision_f=0.7028; val_auroc_f=0.7040\n",
    "trained_till_epoch_f=\"Nan\"; lowest_val_loss_on_epoch_f=\"Nan\"\n",
    "# Unet 2 binary crossentropy loss\n",
    "train_loss_u2_bc=0.0693; train_acc_u2_bc=0.8881; train_f1_u2_bc=0.5682; train_sp_u2_bc=0.8840; train_se_u2_bc=0.8840; train_precision_u2_bc=0.8833; train_auroc_u2_bc=0.8840\n",
    "val_loss_u2_bc=0.2361; val_acc_u2_bc=0.7802; val_f1_u2_bc=0.4762; val_sp_u2_bc=0.8205; val_se_u2_bc=0.8205; val_precision_u2_bc=0.8201; val_auroc_u2_bc=0.8205\n",
    "trained_till_epoch_u2_bc=71; lowest_val_loss_on_epoch_u2_bc=33;\n",
    "# Unet 3 binary crossentropy loss\n",
    "train_loss_u3_bc=0.09351323; train_acc_u3_bc=0.8605181; train_f1_u3_bc=0.5424623; train_sp_u3_bc=0.86103207; train_se_u3_bc=0.86103207; train_precision_u3_bc=0.860599; train_auroc_u3_bc=0.86102957\n",
    "val_loss_u3_bc=0.15823; val_acc_u3_bc=0.83833; val_f1_u3_bc=0.52537; val_sp_u3_bc=0.85357; val_se_u3_bc=0.85357; val_precision_u3_bc=0.85452; val_auroc_u3_bc=0.85358\n",
    "trained_till_epoch_u3_bc=131; lowest_val_loss_on_epoch_u3_bc=29;highest_val_acc_on_epoch_u3_bc=68;highest_val_f1_on_epoch_u3_bc=68;highest_val_sp_on_epoch_u3_bc=109;highest_val_se_on_epoch_u3_bc=109;highest_val_prec_on_epoch_u3_bc=109;highest_val_auroc_on_epoch_u3_bc=109;\n",
    "# Unet 4 binary crossentropy loss\n",
    "train_loss_u4_bc=0.0733392; train_acc_u4_bc=0.88768107; train_f1_u4_bc=0.567843; train_sp_u4_bc=0.88820463; train_se_u4_bc=0.88820463; train_precision_u4_bc=0.88785917; train_auroc_u4_bc=0.8882016\n",
    "val_loss_u4_bc=0.14747; val_acc_u4_bc=0.83952; val_f1_u4_bc=0.52617; val_sp_u4_bc=0.86361; val_se_u4_bc=0.86361; val_precision_u4_bc=0.86396; val_auroc_u4_bc=0.86361\n",
    "trained_till_epoch_u4_bc=35; lowest_val_loss_on_epoch_u4_bc=6; highest_val_acc_on_epoch_u4_bc=30; highest_val_f1_on_epoch_u4_bc=30; highest_val_sp_on_epoch_u4_bc=27; highest_val_se_on_epoch_u4_bc=27;highest_val_prec_on_epoch_u4_bc=27;highest_val_auroc_on_epoch_u4_bc=27\n",
    "\n",
    "# Unet 5 binary crossentropy loss\n",
    "train_loss_u5_bc=0.17507; train_acc_u5_bc=0.74864; train_f1_u5_bc=0.44906; train_sp_u5_bc=0.74322; train_se_u5_bc=0.74322; train_precision_u5_bc=0.74317; train_auroc_u5_bc=0.74322\n",
    "val_loss_u5_bc=0.15751; val_acc_u5_bc=0.7973; val_f1_u5_bc=0.49042; val_sp_u5_bc=0.85111; val_se_u5_bc=0.85111; val_precision_u5_bc=0.8576555; val_auroc_u5_bc=0.85112\n",
    "trained_till_epoch_u5_bc=45; lowest_val_loss_on_epoch_u5_bc=31; highest_val_acc_on_epoch_u5_bc=30; highest_val_f1_on_epoch_u5_bc=30; highest_val_sp_on_epoch_u5_bc=1; highest_val_se_on_epoch_u5_bc=1;highest_val_prec_on_epoch_u5_bc=1;highest_val_auroc_on_epoch_u5_bc=1\n",
    "\n",
    "# Custom net (sine-net) patch 48, patch_num 1500\n",
    "train_loss_c1_bc=0.0811179; train_acc_c1_bc=0.8779; train_f1_c1_bc=0.5594; train_sp_c1_bc=0.8773; train_se_c1_bc=0.8773; train_precision_c1_bc=0.8768; train_auroc_c1_bc=0.8773\n",
    "val_loss_c1_bc=0.14179; val_acc_c1_bc=0.832317; val_f1_c1_bc=0.520198; val_sp_c1_bc=0.85573; val_se_c1_bc=0.85573; val_precision_c1_bc=0.856372; val_auroc_c1_bc=0.855734\n",
    "trained_till_epoch_c1_bc=141; lowest_val_loss_on_epoch_c1_bc=90; highest_val_acc_on_epoch_c1_bc=46; highest_val_f1_on_epoch_c1_bc=46; highest_val_sp_on_epoch_c1_bc=87; highest_val_se_on_epoch_c1_bc=87;highest_val_prec_on_epoch_c1_bc=87;highest_val_auroc_on_epoch_c1_bc=87\n",
    "\n",
    "\n",
    "# Custom2 net patch 128, patch_num 1500\n",
    "train_loss_c2_bc=0.14007; train_acc_c2_bc=0.25734; train_f1_c2_bc=0.13753; train_sp_c2_bc=0.14771; train_se_c2_bc=0.14771; train_precision_c2_bc=0.14771; train_auroc_c2_bc=0.14771\n",
    "val_loss_c2_bc=0.12809; val_acc_c2_bc=0.30818; val_f1_c2_bc=0.16695; val_sp_c2_bc=0.18216; val_se_c2_bc=0.18216; val_precision_c2_bc=0.18216; val_auroc_c2_bc=0.18216\n",
    "trained_till_epoch_c2_bc=41; lowest_val_loss_on_epoch_c2_bc=39; highest_val_acc_on_epoch_c2_bc=2; highest_val_f1_on_epoch_c2_bc=2; highest_val_sp_on_epoch_c2_bc=2; highest_val_se_on_epoch_c2_bc=2;highest_val_prec_on_epoch_c2_bc=2;highest_val_auroc_on_epoch_c2_bc=2\n",
    "\n",
    "\n",
    "# info about nets\n",
    "x = PrettyTable()\n",
    "x.field_names = [f\"Net Config\", \"Net\", \"Number of parameters\", \"comments\"]\n",
    "x.add_row([\"report\", \"UNet1\", \"11,351,706\", \"The Unet architecture which uses residual blocks and residual path and skip connections\"])\n",
    "x.add_row([\"report\", \"UNet2\", \"5,442,250\", \"The same UNet, but replacing all the res blocks with Conv2D\"])\n",
    "x.add_row([\"report\", \"UNet3\", \"1,559,069\", \"This is a basic UNet model without even the linear transform for preprocessing\"])\n",
    "x.add_row([\"report\", \"UNet4\", \"1,559,063\", \"The same basic UNet with linear transform for preprocessing\"])\n",
    "x.add_row([\"report\", \"UNet5\", \"4,489,137\", \"This is the UNet3 plus residual blocks defined in UNet1 instead of conv blocks\"])\n",
    "x.add_row([\"report\", \"custom1\", \"692,759\", \"sine-net paper network but with patch_size 48 and patch_num 1500\"])\n",
    "x.add_row([\"report\", \"custom2\", \"4,715,008\", \"fast and efficient rbvs paper network  with patch_size 128. In the paper parameters are reported to be 10M\"])\n",
    "print(x)\n",
    "\n",
    "# train\n",
    "x = PrettyTable()\n",
    "x.field_names = [f\"Metrics train data\", \"Net\", \"loss_function\", \"train_loss\", \"train_acc\", \"train_f1\", \"train_specificity\", \"train_sensitivity\", \"train_precision\", \"train_auroc\"]\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_bc}\", \"UNet1\", \"binary_crossentropy\", train_loss_bc, train_acc_bc, train_f1_bc, train_sp_bc, train_se_bc, train_precision_bc, train_auroc_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_d}\", \"UNet1\", \"dice_loss\", train_loss_d, train_acc_d, train_f1_d, train_sp_d, train_se_d, train_precision_d, train_auroc_d])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_f}\", \"UNet1\", \"focal_loss\", train_loss_f, train_acc_f, train_f1_f, train_sp_f, train_se_f, train_precision_f, train_auroc_f])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u2_bc}\", \"UNet2\", \"binary_crossentropy\", train_loss_u2_bc, train_acc_u2_bc, train_f1_u2_bc, train_sp_u2_bc, train_se_u2_bc, train_precision_u2_bc, train_auroc_u2_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u3_bc}\", \"UNet3\", \"binary_crossentropy\", train_loss_u3_bc, train_acc_u3_bc, train_f1_u3_bc, train_sp_u3_bc, train_se_u3_bc, train_precision_u3_bc, train_auroc_u3_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u4_bc}\", \"UNet4\", \"binary_crossentropy\", train_loss_u4_bc, train_acc_u4_bc, train_f1_u4_bc, train_sp_u4_bc, train_se_u4_bc, train_precision_u4_bc, train_auroc_u4_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u5_bc}\", \"UNet5\", \"binary_crossentropy\", train_loss_u5_bc, train_acc_u5_bc, train_f1_u5_bc, train_sp_u5_bc, train_se_u5_bc, train_precision_u5_bc, train_auroc_u5_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_c1_bc}\", \"custom1\", \"binary_crossentropy\", train_loss_c1_bc, train_acc_c1_bc, train_f1_c1_bc, train_sp_c1_bc, train_se_c1_bc, train_precision_c1_bc, train_auroc_c1_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_c1_bc}\", \"custom2\", \"binary_crossentropy\", train_loss_c2_bc, train_acc_c2_bc, train_f1_c2_bc, train_sp_c2_bc, train_se_c2_bc, train_precision_c2_bc, train_auroc_c2_bc])\n",
    "print(x)\n",
    "\n",
    "\n",
    "# validation\n",
    "x = PrettyTable()\n",
    "x.field_names = [f\"Metrics val data\", \"Net\", \"loss_function\", \"val_loss\", \"val_acc\", \"val_f1\", \"val_specificity\", \"val_sensitivity\", \"val_precision\", \"val_auroc\"]\n",
    "x.add_row([f\"report on best epoch results\", \"UNet1\", \"binary_crossentropy\", val_loss_bc, val_acc_bc, val_f1_bc, val_sp_bc, val_se_bc, val_precision_bc, val_auroc_bc])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_d}\", \"UNet1\", \"dice_loss\", val_loss_d, val_acc_d, val_f1_d, val_sp_d, val_se_d, val_precision_d, val_auroc_d])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_f}\", \"UNet1\", \"focal_loss\", val_loss_f, val_acc_f, val_f1_f, val_sp_f, val_se_f, val_precision_f, val_auroc_f])\n",
    "x.add_row([f\"report on epoch {trained_till_epoch_u2_bc}\", \"UNet2\", \"binary_crossentropy\", val_loss_u2_bc, val_acc_u2_bc, val_f1_u2_bc, val_sp_u2_bc, val_se_u2_bc, val_precision_u2_bc, val_auroc_u2_bc])\n",
    "x.add_row([f\"report on best epoch results {trained_till_epoch_u3_bc}\", \"UNet3\", \"binary_crossentropy\", val_loss_u3_bc, val_acc_u3_bc, val_f1_u3_bc, val_sp_u3_bc, val_se_u3_bc, val_precision_u3_bc, val_auroc_u3_bc])\n",
    "x.add_row([f\"report on best epoch results\", \"UNet4\", \"binary_crossentropy\", val_loss_u4_bc, val_acc_u4_bc, val_f1_u4_bc, val_sp_u4_bc, val_se_u4_bc, val_precision_u4_bc, val_auroc_u4_bc])\n",
    "x.add_row([f\"report on best epoch results\", \"UNet5\", \"binary_crossentropy\", val_loss_u5_bc, val_acc_u5_bc, val_f1_u5_bc, val_sp_u5_bc, val_se_u5_bc, val_precision_u5_bc, val_auroc_u5_bc])\n",
    "x.add_row([f\"report on best epoch results\", \"custom1\", \"binary_crossentropy\", val_loss_c1_bc, val_acc_c1_bc, val_f1_c1_bc, val_sp_c1_bc, val_se_c1_bc, val_precision_c1_bc, val_auroc_c1_bc])\n",
    "x.add_row([f\"report on best epoch results\", \"custom2\", \"binary_crossentropy\", val_loss_c2_bc, val_acc_c2_bc, val_f1_c2_bc, val_sp_c2_bc, val_se_c2_bc, val_precision_c2_bc, val_auroc_c2_bc])\n",
    "print(x)\n",
    "\n",
    "# epochs info\n",
    "x = PrettyTable()\n",
    "x.field_names = [f\"val metrics changes on epochs\", \"trained epoch\", \"lowest val_loss epoch\", \"highest val_acc epoch\", \"highest val_f1 epoch\", \"highest val_sp epoch\", \"highest val_se epoch\", \"highest val_prec epoch\", \"highest val_auroc epoch\"]\n",
    "x.add_row([\"UNet1_binarycrossentropy\", trained_till_epoch_bc, lowest_val_loss_on_epoch_bc, highest_val_acc_on_epoch_bc, highest_val_f1_on_epoch_bc, highest_val_sp_on_epoch_bc, highest_val_se_on_epoch_bc, highest_val_prec_on_epoch_bc, highest_val_auroc_on_epoch_bc])\n",
    "x.add_row([\"UNet1_dice_loss\", trained_till_epoch_d, lowest_val_loss_on_epoch_d, \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"])\n",
    "x.add_row([\"UNet1_focal_loss\", trained_till_epoch_f, lowest_val_loss_on_epoch_f, \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"])\n",
    "x.add_row([\"UNet2_binarycrossentropy\", trained_till_epoch_u2_bc, lowest_val_loss_on_epoch_u2_bc, \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\", \"NaN\"])\n",
    "x.add_row([\"UNet3_binarycrossentropy\", trained_till_epoch_u3_bc, lowest_val_loss_on_epoch_u3_bc, highest_val_acc_on_epoch_u3_bc, highest_val_f1_on_epoch_u3_bc, highest_val_sp_on_epoch_u3_bc, highest_val_se_on_epoch_u3_bc, highest_val_prec_on_epoch_u3_bc, highest_val_auroc_on_epoch_u3_bc])\n",
    "x.add_row([\"UNet4_binarycrossentropy\", trained_till_epoch_u4_bc, lowest_val_loss_on_epoch_u4_bc, highest_val_acc_on_epoch_u4_bc, highest_val_f1_on_epoch_u4_bc, highest_val_sp_on_epoch_u4_bc, highest_val_se_on_epoch_u4_bc, highest_val_prec_on_epoch_u4_bc, highest_val_auroc_on_epoch_u4_bc])\n",
    "x.add_row([\"UNet5_binarycrossentropy\", trained_till_epoch_u5_bc, lowest_val_loss_on_epoch_u5_bc, highest_val_acc_on_epoch_u5_bc, highest_val_f1_on_epoch_u5_bc, highest_val_sp_on_epoch_u5_bc, highest_val_se_on_epoch_u5_bc, highest_val_prec_on_epoch_u5_bc, highest_val_auroc_on_epoch_u5_bc])\n",
    "x.add_row([\"custom1_binarycrossentropy\", trained_till_epoch_c1_bc, lowest_val_loss_on_epoch_c1_bc, highest_val_acc_on_epoch_c1_bc, highest_val_f1_on_epoch_c1_bc, highest_val_sp_on_epoch_c1_bc, highest_val_se_on_epoch_c1_bc, highest_val_prec_on_epoch_c1_bc, highest_val_auroc_on_epoch_c1_bc])\n",
    "x.add_row([\"custom2_binarycrossentropy\", trained_till_epoch_c2_bc, lowest_val_loss_on_epoch_c2_bc, highest_val_acc_on_epoch_c2_bc, highest_val_f1_on_epoch_c2_bc, highest_val_sp_on_epoch_c2_bc, highest_val_se_on_epoch_c2_bc, highest_val_prec_on_epoch_c2_bc, highest_val_auroc_on_epoch_c2_bc])\n",
    "print(x)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Welcome To Colaboratory",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
